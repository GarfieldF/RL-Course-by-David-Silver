{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36764bitpytorch3conda0092552352d845c1bfe75b50345db5ee",
   "display_name": "Python 3.6.7 64-bit ('pytorch3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing Atari Games with DQN\n",
    "\n",
    "### From Q-table to Q-network\n",
    "\n",
    "Last time, we learned about Q-Learning: an algorithm which produces a Q-table that an agent uses to find the best action to take given a state. The Q-table as a “cheat-sheet” helps us to find the maximum expected future reward of an action, given a current state. This was a good strategy — however, this is not scalable. As we see, producing and updating a Q-table can become ineffective in big state space environments.\n",
    "\n",
    "Today, we’ll create a Deep Q Neural Network. Instead of using a Q-table, we’ll implement a Neural Network that takes a state and approximates Q-values for each action based on that state.\n",
    "\n",
    "\n",
    "\n",
    "### A Quick View into DQN\n",
    "\n",
    "The picture below will be the architecture of our Deep Q Learning. Our Deep Q Neural Network takes a stack of four frames as an input. These pass through its network, and output a vector of Q-values for each action possible in the given state. We need to take the biggest Q-value of this vector to find our best action.\n",
    "\n",
    "In the beginning, the agent does really badly. But over time, it begins to associate frames (states) with best actions to do.\n",
    "\n",
    "paper: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 as cv\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym and Atari Game\n",
    "\n",
    "\n",
    "https://github.com/openai/gym\n",
    "\n",
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.\n",
    "\n",
    "The Atari 2600 (or Atari Video Computer System or Atari VCS before November 1982) is a home video game console from Atari, Inc. Released on September 11, 1977."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for atari game support\n",
    "!pip install gym[atari]\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "frame = env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(frame)\n",
    "\n",
    "totalActions = env.action_space.n\n",
    "\n",
    "print(\"Shape = \",frame.shape)\n",
    "print(\"Action space size: Action  {}\".format(totalActions))\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame Preprocessing\n",
    "\n",
    "Preprocessing is an important step. We want to reduce the complexity of our states to reduce the computation time needed for training.\n",
    "\n",
    "First, we can grayscale each of our states. Color does not add important information (in our case, we just need to find the enemy and kill him, and we don’t need color to find him). This is an important saving, since we reduce our three colors channels (RGB) to 1 (grayscale).\n",
    "\n",
    "Then we reduce the size of the frame, and we stack four sub-frames together. The stack operation will be done later in our code.\n",
    "\n",
    "Why use stack? -- Can you tell me where the ball is going if not use stack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img):\n",
    "    grayImg = cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "    resized = cv.resize(grayImg,(84,110))\n",
    "    preProcessedImg = resized[:84,:84]\n",
    "    \n",
    "    return preProcessedImg\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "newFrame = preprocess(frame)\n",
    "print(newFrame.shape,newFrame.dtype)\n",
    "\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.imshow(newFrame)\n",
    "\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.imshow(frame)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Convolution Networks\n",
    "\n",
    "The frames are processed by three convolution layers. These layers allow you to exploit spatial relationships in images. But also, because frames are stacked together, you can exploit some spatial properties across those frames.\n",
    "\n",
    "Each convolution layer will use RELU as an activation function. RELU has been proven to be a good activation function for convolution layers.\n",
    "\n",
    "We use one fully connected layer with RELU activation function and one output layer (a fully connected layer with a linear activation function) that produces the Q-value estimation for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class atariAgent():\n",
    "    \"\"\" Atari Agent contains the model and functions to predict and train the agent\"\"\"\n",
    "    def __init__(self,totalActions,scope = \"agent\"):\n",
    "        self.scope = scope\n",
    "        self.totalActions = totalActions\n",
    "        with tf.variable_scope(self.scope):\n",
    "            self.QModel()\n",
    "    \n",
    "    def QModel(self):\n",
    "        \"\"\"Contains the model\"\"\"\n",
    "        self.Xin = tf.placeholder(shape=[None,84,84,4],dtype=tf.uint8,name='Xin')\n",
    "        self.y = tf.placeholder(shape=[None],dtype=tf.float32,name='yin')\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32,name='actions')\n",
    "        \n",
    "        X = tf.to_float(self.Xin) / 255.0 # scale to [0,1]\n",
    "        \n",
    "        # model starts\n",
    "        conv1 = tf.contrib.layers.conv2d(X,16,8,4,activation_fn=tf.nn.relu)\n",
    "        \n",
    "        conv2 = tf.contrib.layers.conv2d(conv1,32,4,2,activation_fn=tf.nn.relu)\n",
    "        \n",
    "        convOut = tf.contrib.layers.flatten(conv2)\n",
    "        fc1 = tf.contrib.layers.fully_connected(convOut,256,activation_fn=tf.nn.relu)\n",
    "        self.QValues = tf.contrib.layers.fully_connected(fc1,self.totalActions,activation_fn=None)\n",
    "        \n",
    "        batchSize = tf.shape(self.Xin)[0] \n",
    "        yIndices = tf.range(batchSize) * self.totalActions + self.actions\n",
    "        self.predictedActions = tf.gather(tf.reshape(self.QValues,[-1]),yIndices)\n",
    "        \n",
    "        #c alculates loss function\n",
    "        self.losses = tf.squared_difference(self.y, self.predictedActions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "        \n",
    "        # training step\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025,0.99)\n",
    "        self.trainStep = self.optimizer.minimize(self.loss, global_step=tf.train.get_global_step())\n",
    "        \n",
    "    def play(self,sess,states):\n",
    "        \"\"\"runs the model for the given state and predicts the Q values\"\"\"\n",
    "        return sess.run(self.QValues,{self.Xin : states})\n",
    "        \n",
    "    def train(self,sess,states,y,actions):\n",
    "        \"\"\"Trains the Agent on the given input and target values and returns the loss\n",
    "        \"\"\"\n",
    "        feed_dict = { self.Xin: states, self.y: y, self.actions: actions }\n",
    "        loss, _ = sess.run([self.loss, self.trainStep],feed_dict)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Experience Replay\n",
    "Using experience replay can make more efficient use of observed experience. Experience replay will help us to handle two things:\n",
    "\n",
    "### Avoid forgetting previous experiences\n",
    "At each time step, we receive a tuple (state, action, reward, new_state). We learn from it (we feed the tuple in our neural network), and then throw this experience. Our problem is that we give sequential samples from interactions with the environment to our neural network. And it tends to forget the previous experiences as it overwrites with new experiences For instance, if we are in the first level and then the second (which is totally different), our agent can forget how to behave in the first level.\n",
    "\n",
    "\n",
    "\n",
    "Our solution: create a “replay buffer.” This stores experience tuples while interacting with the environment, and then we sample a small batch of tuple to feed our neural network. As a consequence, it can be more efficient to make use of previous experience, by learning with it multiple times.\n",
    "\n",
    "Think of the replay buffer as a folder where every sheet is an experience tuple. You feed it by interacting with the environment. And then you take some random sheet to feed the neural network. This prevents the network from only learning about what it has immediately done.\n",
    "\n",
    "\n",
    "\n",
    "### Reduce correlations between experiences\n",
    "We have another problem — we know that every action affects the next state. This outputs a sequence of experience tuples which can be highly correlated. If we train the network in sequential order, we risk our agent being influenced by the effect of this correlation.\n",
    "\n",
    "By sampling from the replay buffer at random, we can break this correlation. This prevents action values from oscillating or diverging catastrophically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple,deque\n",
    "\n",
    "def initExperienceReplay(env, initReplaySize: int, cell: namedtuple):\n",
    "    replayBuffer = deque()\n",
    "    state = env.reset()\n",
    "    state = preprocess(state)\n",
    "    state = np.stack([state]*4,axis=2)\n",
    "    print(\"Filling Experience memory of the agent\")\n",
    "    for i in range(initReplaySize):\n",
    "        action = env.action_space.sample()\n",
    "        nextState, reward, isDone, _ = env.step(action)\n",
    "        nextState = preprocess(nextState)\n",
    "        nextState = np.append(state[:,:,1:],nextState[:,:,np.newaxis],axis=2) \n",
    "        replayBuffer.append(cell(state,reward,action,nextState,isDone))\n",
    "        if(isDone):\n",
    "            state = env.reset()\n",
    "            state = preprocess(state)\n",
    "            state = np.stack([state]*4,axis=2)\n",
    "        else:\n",
    "            state = nextState\n",
    "    \n",
    "    env.close()\n",
    "    print(\"Filled memory of size {}\".format(len(replayBuffer)))       \n",
    "    return replayBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Separate Target Network\n",
    "\n",
    "The third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training.\n",
    "\n",
    "## Why not use just use one network for both estimations?\n",
    "\n",
    "The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyParameters(sess,targetModel,QModel):\n",
    "    params1 = [var for var in tf.trainable_variables() if var.name.startswith(targetModel.scope)]\n",
    "    params1 = sorted(params1,key=lambda var: var.name)\n",
    "    params2 = [var for var in tf.trainable_variables() if var.name.startswith(QModel.scope)]\n",
    "    params2 = sorted(params2,key=lambda var: var.name)\n",
    "    copies = []\n",
    "    for p1,p2 in zip(params1,params2):\n",
    "        copy = p1.assign(p2)\n",
    "        copies.append(copy)\n",
    "    sess.run(copies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "\n",
    "Still simply using the  $\\lambda$ -greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EGreedyPolicy(epsilon,QValues):\n",
    "    numActions = QValues.shape[1]\n",
    "    probs = np.ones(numActions, dtype=float) * epsilon / numActions\n",
    "    best_action = np.argmax(QValues)\n",
    "    \n",
    "    probs[best_action] += (1.0 - epsilon)\n",
    "    \n",
    "    optimizedAction = np.random.choice(numActions,p=probs)\n",
    "    return optimizedAction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpisodes = 5  # determines the number of episode\n",
    "initReplaySize = 5000\n",
    "replaySize = 10000\n",
    "batchSize = 32\n",
    "startE = 1.0  # for epsilon decay, start from 1.0\n",
    "endE = 0.1  # for epsilon decay, end with 0.1\n",
    "annealingSteps = 5000  # for epsilon decay, determines the decay step\n",
    "copyFrequency = 10000  # determines the update frequency\n",
    "videoFrequency = 5000\n",
    "discountFactor = 0.99  # discounted factor\n",
    "checkpointDir = \"checkpoint\"\n",
    "monitorDir = \"monitor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor  # an environment wrapper, for recording\n",
    "\n",
    "def trainAgent():\n",
    "    #start environment\n",
    "    env = gym.make('BreakoutDeterministic-v4')\n",
    "    \n",
    "    totalActions = env.action_space.n\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # initialize target and evaluation model\n",
    "    targetModel = atariAgent(totalActions,scope=\"targetModel\")\n",
    "    QModel = atariAgent(totalActions,scope=\"QModel\")\n",
    "    \n",
    "    if not os.path.exists(checkpointDir):\n",
    "        os.makedirs(checkpointDir)\n",
    "    if not os.path.exists(monitorDir):\n",
    "        os.makedirs(monitorDir)\n",
    "    \n",
    "    checkpoint = os.path.join(checkpointDir,\"model\")\n",
    "    monitor = os.path.join(monitorDir,\"game\")\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        env = Monitor(env, directory=monitor, video_callable=lambda totalStep: totalStep % \\\n",
    "                      videoFrequency==0, resume=True)\n",
    "        \n",
    "        state = env.reset()\n",
    "        \n",
    "        ckpt = tf.train.latest_checkpoint(checkpointDir)\n",
    "        if ckpt:\n",
    "            saver.restore(sess,ckpt)\n",
    "            totalStep = 0\n",
    "            print(\"Existing checkpoint {} restored...\".format(ckpt))\n",
    "        else:\n",
    "            totalStep = 0\n",
    "        \n",
    "        cell = namedtuple(\"cell\",\"state reward action nextState isDone\")\n",
    "        \n",
    "        replayMemory = initExperienceReplay(env,initReplaySize,cell)\n",
    "        \n",
    "        epsilonValues = np.linspace(startE,endE,num=annealingSteps)\n",
    "        \n",
    "        episodeLengths = []\n",
    "        episodeRewards = []\n",
    "        \n",
    "        print(\"\\n---------- Main Loop ----------\")\n",
    "        \n",
    "        for episode in range(numEpisodes):\n",
    "            state = env.reset()\n",
    "            state = preprocess(state)\n",
    "            state = np.stack([state] * 4, axis=2)\n",
    "            \n",
    "            loss = None\n",
    "            episodeLength = 0\n",
    "            totalReward = 0.\n",
    "            \n",
    "            while(True):\n",
    "                if(totalStep%copyFrequency == 0):\n",
    "                    copyParameters(sess,targetModel,QModel)\n",
    "                    print(\"\\nTarget Model updated...\")\n",
    "\n",
    "                epsilon = epsilonValues[min(totalStep, annealingSteps-1)]\n",
    "                QValues = QModel.play(sess,np.expand_dims(state,0))\n",
    "                bestAction = EGreedyPolicy(epsilon,QValues)\n",
    "\n",
    "                nextState,reward,isDone,_ = env.step(bestAction)\n",
    "                nextState = preprocess(nextState)\n",
    "                nextState = np.append(state[:,:,1:],nextState[:,:,np.newaxis],axis=2)\n",
    "                \n",
    "                totalReward += reward\n",
    "                \n",
    "                if(len(replayMemory) == replaySize):\n",
    "                    replayMemory.popleft()\n",
    "                \n",
    "                replayMemory.append(cell(state,reward,bestAction,nextState,isDone))\n",
    "\n",
    "                indices = np.random.choice(len(replayMemory)-1,batchSize,replace=False)\n",
    "                batch = [replayMemory[i] for i in indices]\n",
    "                states,rewards,actions,nextStates,isDones = map(np.array,zip(*batch))\n",
    "                \n",
    "                # targetmodel prediction\n",
    "                tQValues = targetModel.play(sess,nextStates)\n",
    "                targetY = rewards + (1 - isDones) * discountFactor * np.amax(tQValues,axis=1)\n",
    "\n",
    "                # gradient descent step\n",
    "                loss = QModel.train(sess,states,targetY,actions)\n",
    "                episodeLength += 1\n",
    "                totalStep += 1\n",
    "                \n",
    "                if(isDone):\n",
    "                    episodeRewards.append(reward)\n",
    "                    episodeLengths.append(episodeLength)\n",
    "                    print(\"\\r[Episode {}]\\tGlobal step: {} Final reward: {} Total reward: {} episode length: {} loss: {}\"\\\n",
    "                          .format(episode,totalStep,reward,totalReward,episodeLength,loss), end=\"\", flush=True)\n",
    "                    \n",
    "                    saver.save(tf.get_default_session(), checkpoint)\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    state = nextState\n",
    "                    totalStep += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready for training\n",
    "!apt install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAgent()"
   ]
  }
 ]
}